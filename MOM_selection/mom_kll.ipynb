{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import sqrt\n",
    "from numpy.random import multivariate_normal, randn\n",
    "import math\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import linear_model\n",
    "import multiprocessing\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n",
      "Computing estimators on subsamples\n"
     ]
    }
   ],
   "source": [
    "#n_features = 30\n",
    "#sparsity = 10\n",
    "#sigma = 1\n",
    "\n",
    "#N = 1000 # size of dataset\n",
    "#V = 40 # number of blocks for median\n",
    "\n",
    "#K0 = math.ceil(math.log(V/3,2)+2)\n",
    "# Kmax = max(3, math.ceil(math.log(V/3,2)+1))\n",
    "#Kmax = 4\n",
    "\n",
    "#grid_lamda = np.exp(np.arange(-2,4,.5))\n",
    "#outliers_plus_heavytail_range = np.arange(0,151,2)\n",
    "\n",
    "#E = 200 # number of experiments (for averaging)\n",
    "\n",
    "#Quick test\n",
    "\n",
    "n_features = 20\n",
    "sparsity = 5\n",
    "sigma = 1\n",
    "N = 100 # size of dataset\n",
    "V = 10 # number of blocks for median\n",
    "K0 = math.ceil(math.log(V/3,2)+2)\n",
    "# Kmax = max(3, math.ceil(math.log(V/3,2)+1))\n",
    "Kmax = 3\n",
    "grid_lamda = np.exp(np.arange(-2,4,1))\n",
    "outliers_plus_heavytail_range = np.arange(0,100,10)\n",
    "E = 1 # number of experiments (for averaging)\n",
    "# CALCUL EN PARALELLE SUR 12 COEURS\n",
    "parallel = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Preliminary computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "subsamples = {}\n",
    "for K in range(3,math.floor(math.log(N, 2))+1):\n",
    "    for k in range(1,2**K+1):\n",
    "        subsamples[(K,k)] = np.arange(math.floor((k-1)*N/2**K),math.floor(k*N/2**K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mom_partition = set()\n",
    "K = math.ceil(math.log(V/3,2)+2)\n",
    "for k in range(1,2**K+1):\n",
    "    mom_partition.add((K,k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Data generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def beta_func(n_features, sparsity):\n",
    "    idx = np.arange(n_features)\n",
    "    beta = (n_features/10)*(-1) ** (abs(idx - 1)) * np.exp(-idx / 10.)\n",
    "    sel = np.random.permutation(n_features)\n",
    "    sel1 = sel[0:int(sparsity/4)]\n",
    "    beta[sel1] = 10\n",
    "    sel11 = sel[int(sparsity/4):int(sparsity/2)]\n",
    "    beta[sel11] = -10\n",
    "    sel0 = sel[sparsity:]\n",
    "    beta[sel0] = 0.\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def data1(n_samples, beta, sigma):\n",
    "    n_features = beta.size\n",
    "    cov = np.identity(n_features)\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    Y = X.dot(beta) + sigma * randn(n_samples)\n",
    "    return Y, X\n",
    "\n",
    "def data2(n_outliers, n_features, type_outliers = 1, beta = 1, rho=1):\n",
    "    if type_outliers == 1:\n",
    "        Y = np.ones(n_outliers)\n",
    "        X = np.ones((n_outliers, n_features))\n",
    "    elif type_outliers == 2:\n",
    "        Y = 10000*np.ones(n_outliers)\n",
    "        X = np.ones((n_outliers, n_features))\n",
    "    elif type_outliers == 3:\n",
    "        Y = np.random.randint(2, size = n_outliers)\n",
    "        X = np.random.rand(n_outliers, n_features)\n",
    "    else:\n",
    "        cov = np.identity(n_features)\n",
    "        X = feature_mat(n_features, n_outliers, rho)\n",
    "        Y = X.dot(beta) + sigma * randn(n_samples)\n",
    "    return Y, X\n",
    "\n",
    "def data3(n_heavy_tail, beta, deg = 2):\n",
    "    n_features = beta.size\n",
    "    cov = np.identity(n_features)\n",
    "    X = multivariate_normal(np.zeros(n_features), cov, size=n_heavy_tail)\n",
    "    Y = X.dot(beta) + np.random.standard_t(deg, size=n_heavy_tail)\n",
    "    return Y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def data_merge_with_outlier_info(Y1, X1, Y2, X2, Y3, X3):\n",
    "    Y = np.concatenate((Y1, Y2, Y3), axis=0)\n",
    "    X = np.concatenate((X1, X2, X3), axis=0)\n",
    "    outlier1p = np.concatenate((np.full(Y1.size,False), np.full(Y2.size,True), np.full(Y3.size, False)))\n",
    "    outlier2p = np.concatenate((np.full(Y1.size,False), np.full(Y2.size,False), np.full(Y3.size, True)))\n",
    "    return shuffle(Y, X, outlier1p, outlier2p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mom_generate_data(n_total, n_outliers, n_heavy_tail):\n",
    "    n_samples = n_total - n_outliers - n_heavy_tail\n",
    "    beta_0 = beta_func(n_features, sparsity)\n",
    "    y1, X1 = data1(n_samples, beta_0,  sigma)\n",
    "    y2, X2 = data2(n_outliers, n_features, type_outliers = 2, beta = 1, rho=1)\n",
    "    y3, X3 = data3(n_heavy_tail, beta_0, deg = 2)\n",
    "    y, X, outlier1p, outlier2p = data_merge_with_outlier_info(y1, X1, y2, X2, y3, X3)\n",
    "\n",
    "    beta_0 = np.matrix(beta_0).T\n",
    "    y = np.matrix(y).T\n",
    "    X = np.matrix(X)\n",
    "    return beta_0, y, X, outlier1p, outlier2p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# MOM Selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mom_number_of_hyperparameters_m(N, grid_lamda, Kmax):\n",
    "    return grid_lamda.size * 8 * (2 ** (Kmax - 2) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mom_decompose_hyperparameter_m(ind_m, N, grid_lamda):\n",
    "    # lamda varie d'abord: cinq blocs B consécutifs sont égaux\n",
    "    # k varie de 1 à 2^K\n",
    "    # K varie de 3 à Kmax\n",
    "    # indice du lamda correspondant\n",
    "    ind_lamda = ind_m % grid_lamda.size\n",
    "    # \"indice\" du block correspondant\n",
    "    ind_wo_lamda = math.floor(ind_m / grid_lamda.size)\n",
    "    K = math.floor(math.log(2**3+ind_wo_lamda, 2))\n",
    "    k = ind_wo_lamda - 8*((2**(K-3)-1))+1\n",
    "    \n",
    "    return ind_lamda, K, k, subsamples[(K,k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mom_intersecting_blocks(K,k,K0):\n",
    "    if K0 <= K:\n",
    "        return {(K0,1+math.floor((k-1)/2**(K-K0)))}\n",
    "    else:\n",
    "        return {(K0, k0) for k0 in range(2**(K0-K)*(k-1), 2**(K0-K)*k)}\n",
    "\n",
    "    mom_intersecting_blocks(1,1,K0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def mom_estimator_selection(X, y, beta_0, V, grid_lamda, Kmax, outlier1p, outlier2p):\n",
    "    global K0, mom_partition\n",
    "    \n",
    "    N = y.size\n",
    "    M = mom_number_of_hyperparameters_m(N, grid_lamda, Kmax)\n",
    "    if V > (N/8):\n",
    "        print('V larger than N/8. Correcting')\n",
    "        V = N/8\n",
    "        # un truc où on peut append:\n",
    "        # blocks = []\n",
    "    estimators = []\n",
    "    estimators_errors = np.zeros(M)\n",
    "    empirical_errors_on_blocks = {}\n",
    "    total_empirical_errors = np.zeros(M)\n",
    "    nb_outliers_in_subsamples = [] \n",
    "    outlierp = list(np.logical_or(outlier1p,outlier2p))\n",
    "\n",
    "    print('Computing estimators on subsamples')\n",
    "    for ind_m in np.arange(0, M):\n",
    "        # print('computing estimator for ind_m=', ind_m, '... ')\n",
    "        ind_lamda, K, k, data_ind = mom_decompose_hyperparameter_m(ind_m, N, grid_lamda)\n",
    "        nb_outliers_in_subsamples.append(sum(outlierp[data] for data in data_ind))\n",
    "        # _, _, estimator = MOM_LASSO.MOM_ADMM(X[data_ind], y[data_ind], beta_0, 1, 100, grid_lamda[ind_lamda])\n",
    "        lasso = linear_model.Lasso(alpha=grid_lamda[ind_lamda], fit_intercept=False)\n",
    "        lasso.fit(X[data_ind], y[data_ind])\n",
    "        estimator = np.transpose([lasso.coef_])\n",
    "\n",
    "        estimators.append(estimator)\n",
    "        estimators_errors[ind_m] = np.linalg.norm(beta_0-estimator)\n",
    "        # compute empirical error on each test block\n",
    "        for T in mom_partition:\n",
    "            subsample = subsamples[T]\n",
    "            empirical_errors_on_blocks[(ind_m,T)] = np.linalg.norm(X[subsample]*estimator-y[subsample])\n",
    "            #best estimator?\n",
    "    best_estimator_ind = np.argmin(estimators_errors)\n",
    "    best_estimator = estimators[best_estimator_ind]\n",
    "    \n",
    "    print('Computing MOM-selection')\n",
    "    max_over_m_prime = np.zeros(M)\n",
    "\n",
    "    for ind_m in np.arange(0,M):\n",
    "        medians = np.zeros(M)\n",
    "        _, K, k, _ = mom_decompose_hyperparameter_m(ind_m, N, grid_lamda)\n",
    "        # print('comparing estimators for ind_m=', ind_m)\n",
    "        for ind_m_prime in np.arange(0,M):\n",
    "            _, K_prime, k_prime, _ = mom_decompose_hyperparameter_m(ind_m_prime, N, grid_lamda)\n",
    "            # comparison partition\n",
    "            comparison_partition = mom_partition.copy()\n",
    "            comparison_partition = comparison_partition - mom_intersecting_blocks(K,k,K0)\n",
    "            comparison_partition = comparison_partition - mom_intersecting_blocks(K_prime,k_prime,K0)\n",
    "            if len(comparison_partition) < V:\n",
    "                raise ValueError('Problème: la parition de test est trop petite')\n",
    "            while len(comparison_partition) > V:\n",
    "                comparison_partition.pop()\n",
    "                # la mediane sur V\n",
    "            empirical_diffs = []\n",
    "            \n",
    "            for T in comparison_partition:\n",
    "                empirical_diffs.append(empirical_errors_on_blocks[(ind_m,T)]-empirical_errors_on_blocks[(ind_m_prime,T)])\n",
    "                # median over v\n",
    "            medians[ind_m_prime] = np.median(empirical_diffs)\n",
    "            # max over m_prime\n",
    "        max_over_m_prime[ind_m] = max(medians)\n",
    "        # argmin over m\n",
    "    selected_m_ind = np.argmin(max_over_m_prime)\n",
    "    selected_estimator = estimators[selected_m_ind]\n",
    "    return estimators, estimators_errors, selected_m_ind, selected_estimator, best_estimator_ind, best_estimator, nb_outliers_in_subsamples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def compute_mom(nb_outliers_plus_heavytail):\n",
    "    global subsamples, mom_partition\n",
    "    beta_0, y, X, outlier1p, outlier2p = mom_generate_data(N, int(nb_outliers_plus_heavytail/2), int(nb_outliers_plus_heavytail/2))\n",
    "    estimators, estimators_errors, selected_m_ind, selected_estimator, best_estimator_ind, best_estimator, nb_outliers_in_subsamples = mom_estimator_selection(X, y, beta_0, V, grid_lamda, Kmax, outlier1p, outlier2p)\n",
    "\n",
    "    selected_estimator_error = np.linalg.norm(beta_0-selected_estimator)\n",
    "    \n",
    "    selected_ind_lamda, _, _, selected_data_ind = mom_decompose_hyperparameter_m(selected_m_ind, N, grid_lamda)\n",
    "    best_estimator_ind_lamda, _, _, best_estimator_data_ind = mom_decompose_hyperparameter_m(best_estimator_ind, N, grid_lamda)\n",
    "\n",
    "    nb_outliers2_in_selected = sum(outlier2p[data] for data in selected_data_ind)\n",
    "    nb_outliers2_in_best = sum(outlier2p[data] for data in best_estimator_data_ind)\n",
    "    \n",
    "    nb_outliers1_in_selected = sum(outlier1p[data] for data in selected_data_ind)\n",
    "    nb_outliers1_in_best = sum(outlier1p[data] for data in best_estimator_data_ind)\n",
    "    \n",
    "    nb_with_no_outlier = nb_outliers_in_subsamples.count(0)/len(grid_lamda)\n",
    "    lowest_error_among_computed = min(estimators_errors)\n",
    "\n",
    "    basic_estimators_errors = []\n",
    "    for lamda in grid_lamda:\n",
    "        # _, _, estimator = MOM_LASSO.MOM_ADMM(X, y, beta_0, 1, 100, lamda)\n",
    "        lasso = linear_model.Lasso(alpha=lamda, fit_intercept=False)\n",
    "        lasso.fit(X, y)\n",
    "        estimator = np.transpose([lasso.coef_])\n",
    "\n",
    "        basic_estimators_errors.append(np.linalg.norm(beta_0-estimator))\n",
    "        lowest_error_among_basic = min(basic_estimators_errors)\n",
    "\n",
    "    return selected_estimator_error, nb_outliers1_in_selected, nb_outliers1_in_best, nb_outliers2_in_selected, nb_outliers2_in_best, nb_with_no_outlier, lowest_error_among_computed, lowest_error_among_basic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/guillaume/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/guillaume/anaconda3/lib/python3.7/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<ipython-input-13-9d70f9fe577a>\", line 4, in compute_mom\n    estimators, estimators_errors, selected_m_ind, selected_estimator, best_estimator_ind, best_estimator, nb_outliers_in_subsamples = mom_estimator_selection(X, y, beta_0, V, grid_lamda, Kmax, outlier1p, outlier2p)\n  File \"<ipython-input-12-f9ff80ed8436>\", line 22, in mom_estimator_selection\n    nb_outliers_in_subsamples.append(sum(outlierp[data] for data in data_ind))\n  File \"<ipython-input-12-f9ff80ed8436>\", line 22, in <genexpr>\n    nb_outliers_in_subsamples.append(sum(outlierp[data] for data in data_ind))\nIndexError: list index out of range\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2ca332665cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlowest_error_among_basic_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mselected_estimators_errors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_outliers1_in_selected_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_outliers1_in_best_estimator_subsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_outliers2_in_selected_blocks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_outliers2_in_best_estimator_subsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_subsamples_with_no_outlier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowest_error_among_computed_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlowest_error_among_basic_estimators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_mom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutliers_plus_heavytail_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mselected_estimators_errors_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_estimators_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         '''\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pool = multiprocessing.Pool(12)\n",
    "\n",
    "selected_estimators_errors_all = []\n",
    "nb_outliers1_in_selected_blocks_all = []\n",
    "nb_outliers1_in_best_estimator_subsample_all = []\n",
    "nb_outliers2_in_selected_blocks_all = []\n",
    "nb_outliers2_in_best_estimator_subsample_all = []\n",
    "nb_subsamples_with_no_outlier_all = []\n",
    "lowest_error_among_computed_estimators_all = []\n",
    "lowest_error_among_basic_estimators_all = []\n",
    "\n",
    "\n",
    "for experiment in range(E):\n",
    "    selected_estimators_errors = []\n",
    "    nb_outliers1_in_selected_blocks = []\n",
    "    nb_outliers1_in_best_estimator_subsample = []\n",
    "    nb_outliers2_in_selected_blocks = []\n",
    "    nb_outliers2_in_best_estimator_subsample = []\n",
    "    nb_subsamples_with_no_outlier = []\n",
    "    lowest_error_among_computed_estimators = []\n",
    "    lowest_error_among_basic_estimators = []\n",
    "\n",
    "    selected_estimators_errors, nb_outliers1_in_selected_blocks, nb_outliers1_in_best_estimator_subsample, nb_outliers2_in_selected_blocks, nb_outliers2_in_best_estimator_subsample, nb_subsamples_with_no_outlier, lowest_error_among_computed_estimators, lowest_error_among_basic_estimators = map(np.array, zip(*pool.map(compute_mom, outliers_plus_heavytail_range)))\n",
    "\n",
    "    selected_estimators_errors_all.append(selected_estimators_errors)\n",
    "    nb_outliers1_in_selected_blocks_all.append(nb_outliers1_in_selected_blocks)\n",
    "    nb_outliers1_in_best_estimator_subsample_all.append(nb_outliers1_in_best_estimator_subsample)\n",
    "    nb_outliers2_in_selected_blocks_all.append(nb_outliers2_in_selected_blocks)\n",
    "    nb_outliers2_in_best_estimator_subsample_all.append(nb_outliers2_in_best_estimator_subsample)\n",
    "    nb_subsamples_with_no_outlier_all.append(nb_subsamples_with_no_outlier)\n",
    "    lowest_error_among_computed_estimators_all.append(lowest_error_among_computed_estimators)\n",
    "    lowest_error_among_basic_estimators_all.append(lowest_error_among_basic_estimators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "fig_size = [6,4]\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "xticks_range = range(0,max(outliers_plus_heavytail_range)+1,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nswoo = np.array(nb_subsamples_with_no_outlier_all)\n",
    "plt.plot(outliers_plus_heavytail_range, nswoo.mean(axis=0))\n",
    "plt.fill_between(outliers_plus_heavytail_range,np.percentile(nswoo, 2.5, axis=0),np.percentile(nswoo, 97.5, axis=0), alpha=.25)\n",
    "plt.xticks(xticks_range)\n",
    "plt.xlabel('Number of outliers')\n",
    "plt.ylabel('Number of subsamples with no outlier')\n",
    "plt.savefig(\"number_of_subsamples_without_outliers.png\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "no1isb = np.array(nb_outliers1_in_selected_blocks_all)\n",
    "no1ibes = np.array(nb_outliers1_in_best_estimator_subsample_all)\n",
    "no2isb = np.array(nb_outliers2_in_selected_blocks_all)\n",
    "no2ibes = np.array(nb_outliers2_in_best_estimator_subsample_all)\n",
    "\n",
    "plt2.stackplot(outliers_plus_heavytail_range, no1isb.mean(axis=0),no2isb.mean(axis=0))\n",
    "plt2.xticks(xticks_range)\n",
    "plt2.xlabel('Number of outliers')\n",
    "plt2.ylabel('Number of outliers in subsample of selected estimator')\n",
    "plt2.legend(['Hard outliers', 'Heavy tails'], numpoints = 1, loc='upper left')\n",
    "plt2.savefig(\"number_of_outliers_in_subsample_of_selected_estimator.png\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "no1ibes = np.array(nb_outliers1_in_best_estimator_subsample_all)\n",
    "no2ibes = np.array(nb_outliers2_in_best_estimator_subsample_all)\n",
    "\n",
    "plt2.stackplot(outliers_plus_heavytail_range, no1ibes.mean(axis=0),no2ibes.mean(axis=0))\n",
    "plt2.xticks(xticks_range)\n",
    "plt2.xlabel('Number of outliers')\n",
    "plt2.ylabel('Number of outliers in subsample of best estimator')\n",
    "plt2.legend(['Hard outliers', 'Heavy tails'], numpoints = 1, loc='upper left')\n",
    "plt2.savefig(\"number_of_outliers_in_subsample_of_best_estimator.png\", dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "see = np.array(selected_estimators_errors_all)\n",
    "leace = np.array(lowest_error_among_computed_estimators_all)\n",
    "leabe = np.array(lowest_error_among_basic_estimators_all)\n",
    "\n",
    "plt.fill_between(outliers_plus_heavytail_range, np.log10(np.percentile(see, 2.5, axis=0)),np.log10(np.percentile(see, 97.5, axis=0)), alpha=.25, color='c')\n",
    "\n",
    "plt.fill_between(outliers_plus_heavytail_range, np.log10(np.percentile(leace, 2.5, axis=0)),np.log10(np.percentile(leace, 97.5, axis=0)), alpha=.25, color='k')\n",
    "\n",
    "plt.fill_between(outliers_plus_heavytail_range,np.log10(np.percentile(leabe, 2.5, axis=0)), np.log10(np.percentile(leabe, 97.5, axis=0)), alpha=.25)\n",
    "\n",
    "plt.plot(outliers_plus_heavytail_range, np.log10(see.mean(axis=0)), 'c',\n",
    "         outliers_plus_heavytail_range, np.log10(leace.mean(axis=0)), 'k',\n",
    "         outliers_plus_heavytail_range, np.log10(leabe.mean(axis=0))\n",
    ")\n",
    "\n",
    "plt.xticks(xticks_range)\n",
    "plt.xlabel('Number of outliers')\n",
    "plt.ylabel('log10(error)')\n",
    "plt.legend(('Selected estimator', 'Best estimator', 'Best basic estimator'), numpoints = 1, loc='center right')\n",
    "plt.savefig(\"accuracy.png\", dpi=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "name": "mom_kll.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
